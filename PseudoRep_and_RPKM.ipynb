{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf70c62-e45a-4c7c-b586-4f3fb4f4623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IMPORT LIBS AND FXS ####\n",
    "import os, sys, glob, subprocess\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "np.random.seed(31415)\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import pysam\n",
    "from pyfaidx import Fasta\n",
    "from Bio.Seq import Seq\n",
    "# Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## DEFINE CUSTOM FUNCTIONS\n",
    "\n",
    "# PREPROCESSING FUNCTIONS\n",
    "def count_mapped_reads(bam_file):\n",
    "    cmd = f\"samtools view -c -F 4 -@ 12 {bam_file}\"\n",
    "    return int(subprocess.check_output(cmd, shell=True).decode().strip())\n",
    "    \n",
    "def sort_bed_file(input_bed_path, output_bed_path, window = 0):\n",
    "    \"\"\"\n",
    "    Sorts a BED file by chromosome and start position, placing numeric chromosomes first,\n",
    "    followed by X, Y, and M chromosomes.\n",
    "    \"window\" parameter (bp) allows modification of flanking window around previous regex search.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the BED file\n",
    "    bed_df = pd.read_csv(input_bed_path, sep=\"\\t\", header=None, names=[\"chr\", \"start\", \"end\", \"hit\"])\n",
    "\n",
    "    # Filter for chromosomes containing 'chr'\n",
    "    bed_df = bed_df.loc[bed_df[\"chr\"].str.contains(\"chr\")]\n",
    "\n",
    "    # Custom sorting key\n",
    "    def chromosome_sort_key(chrom):\n",
    "        chrom = chrom.replace(\"chr\", \"\")\n",
    "        if chrom.isdigit():\n",
    "            return (0, int(chrom))  # Numeric chromosomes\n",
    "        elif chrom == \"X\":\n",
    "            return (1, 23)  # X chromosome\n",
    "        elif chrom == \"Y\":\n",
    "            return (2, 24)  # Y chromosome\n",
    "        elif chrom == \"M\":\n",
    "            return (3, 25)  # Mitochondrial chromosome\n",
    "        else:\n",
    "            return (4, chrom)  # Any other chromosomes\n",
    "\n",
    "    # Sort DataFrame using the custom key\n",
    "    bed_df['chr_sort_key'] = bed_df['chr'].map(chromosome_sort_key)\n",
    "    bed_df[\"start\"] = bed_df[\"start\"].apply(lambda x: x-window)\n",
    "    bed_df[\"end\"] = bed_df[\"end\"].apply(lambda x: x+window)\n",
    "    bed_df.sort_values(by=[\"chr_sort_key\", \"start\"], inplace=True)\n",
    "\n",
    "    # Drop the temp sort key column\n",
    "    bed_df.drop(columns=['chr_sort_key'], inplace=True)\n",
    "\n",
    "    # Save the sorted BED file\n",
    "    bed_df[[\"chr\", \"start\", \"end\", \"hit\"]].to_csv(output_bed_path, index=False, header=False, sep=\"\\t\")\n",
    "    \n",
    "# SIGNAL-TO-NOISE RATIO\n",
    "def calculate_signal_to_noise(bam_path, peaks_df):\n",
    "    bamfile = pysam.AlignmentFile(bam_path, \"rb\")\n",
    "    \n",
    "    # Number of reads in peaks\n",
    "    reads_in_peaks = 0\n",
    "    for index, row in peaks_df.iterrows():\n",
    "        if not row['Chrom'].startswith(\"chr\"): # SKIP Alternate Assemblies\n",
    "            continue\n",
    "        reads_in_peaks += bamfile.count(row['Chrom'], row['Start'], row['End'])\n",
    "    \n",
    "    # Total number of reads in BAM file\n",
    "    total_reads = bamfile.mapped\n",
    "    \n",
    "    # Number of reads outside peaks\n",
    "    reads_outside_peaks = total_reads - reads_in_peaks\n",
    "    \n",
    "    bamfile.close()\n",
    "    \n",
    "    # Calculate Signal-to-Noise ratio\n",
    "    signal_to_noise = reads_in_peaks / reads_outside_peaks if reads_outside_peaks > 0 else 0\n",
    "    return signal_to_noise\n",
    "\n",
    "## RPKM PROCESSING\n",
    "# Pattern Matching\n",
    "def find_hits_and_create_bed(editor, regexes, window = 0, bed_path = f\"hits.bed\"):\n",
    "    '''\n",
    "    Locate Seed&PAM matches in reference genome using regex, then output regions in bed file.\n",
    "    '''\n",
    "    genome_path = \"/groups/doudna/team_resources/shared_databases/human_reference_genomes/hg38/GENCODE_GRCh38p14_v44/GRCh38_PRI.fa\"\n",
    "    fasta = Fasta(genome_path)\n",
    "    hits = []\n",
    "\n",
    "    for chrom in fasta.keys():\n",
    "        sequence = str(fasta[chrom])\n",
    "        for regex in regexes:\n",
    "            for match in re.finditer(regex, sequence):\n",
    "                start = max(match.start() - window, 0)\n",
    "                end = min(match.end() + window, len(sequence))\n",
    "                hits.append((chrom, start, end, match.group()))\n",
    "\n",
    "    with open(bed_path, 'w') as f:\n",
    "        for chrom, start, end, match in hits:\n",
    "            f.write(f\"{chrom}\\t{start}\\t{end}\\t{match}\\n\")\n",
    "\n",
    "def reverse_complement(seq):\n",
    "    return str(Seq(seq).reverse_complement())\n",
    "            \n",
    "def calculate_rpkm(file_path, scale_factor):\n",
    "    '''\n",
    "    Calculate rpkm from coverage bed file.\n",
    "    '''\n",
    "    if \"complement\" in file_path:\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"chr\", \"start\", \"end\", \"reads\"])\n",
    "    else:\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"chr\", \"start\", \"end\", \"hit\", \"reads\"])\n",
    "\n",
    "    df['rpkm'] = df['reads'] / (df['end'] - df['start']) * scale_factor\n",
    "    return df\n",
    "    \n",
    "def process_chip_data(glob_path, mapped_bam_reads, experimental_setup):\n",
    "    '''\n",
    "    Collect RPKM for samples based on \"experimental_setup\" dictionary\n",
    "    '''\n",
    "    treatment_dict = {}\n",
    "\n",
    "    if type(glob_path) == list:\n",
    "        to_process = glob_path\n",
    "    else:\n",
    "        to_process = glob.glob(glob_path)\n",
    "    \n",
    "    for hit_cov in to_process:\n",
    "        # Clean file name\n",
    "        base_name = hit_cov.split(\"/\")[-1].split(\"_sorted\")[0]\n",
    "        casid = base_name.split(\"_pseudo\")[0].replace(\"ChIP__KMW_060324_\", \"\").replace(\"_UCLA\", \"\").replace(\"d\", \"\")\n",
    "        regexid = hit_cov.split(\"/\")[-1].split(\"___\")[1]\n",
    "        treatment = f\"{casid}_{regexid}\"\n",
    "        \n",
    "        print(f\"# Processing: {base_name}\")\n",
    "        if os.path.exists(hit_cov):\n",
    "            print(f\"# Processing: {hit_cov}\\n{base_name}\")\n",
    "            coverage_df = calculate_rpkm(hit_cov, mapped_bam_reads[base_name] / 1e6)\n",
    "            coverage_df.columns = [f\"{col}_{base_name}___{treatment}\" for col in coverage_df.columns]\n",
    "            \n",
    "            if treatment in treatment_dict:\n",
    "                treatment_dict[treatment].append(coverage_df)\n",
    "            else:\n",
    "                treatment_dict[treatment] = [coverage_df]\n",
    "\n",
    "    results = []\n",
    "    for treatment, files_list in experimental_setup.items():\n",
    "        treatment_dfs = []\n",
    "        control_dfs = []\n",
    "        print(f\"# REPLICATES: {treatment}, {files_list}\")\n",
    "        \n",
    "        # Process treatment replicates\n",
    "        if len(files_list)>0:\n",
    "            dfs = treatment_dict.get(files_list[0], [])\n",
    "            concat_df_trt = pd.concat(dfs, axis=1)\n",
    "            mean_rpkm = concat_df_trt.filter(like='rpkm').mean(axis=1)\n",
    "            treatment_dfs.append(mean_rpkm)\n",
    "        \n",
    "        # Optionally process control/background replicates\n",
    "        if len(files_list)>1:\n",
    "            dfs = treatment_dict.get(files_list[1], [])\n",
    "            concat_df = pd.concat(dfs, axis=1)\n",
    "            mean_rpkm = concat_df.filter(like='rpkm').mean(axis=1)\n",
    "            control_dfs.append(mean_rpkm)\n",
    "\n",
    "        # Calculate statistics\n",
    "        treatment_avg = concat_df_trt.filter(like='rpkm').mean(axis=1) \n",
    "        control_avg = control_dfs[0]\n",
    "\n",
    "        # Subtract control from treatment if control data exists\n",
    "        if not control_dfs:\n",
    "            treatment_std = concat_df_trt.filter(like='rpkm').mean().std() if treatment_dfs else None\n",
    "            results.append({'Treatment': treatment, 'Avg_RPKM': treatment_avg.mean(), 'Std_RPKM': treatment_std})\n",
    "        else:\n",
    "            rpkm_df = concat_df_trt.filter(like='rpkm')\n",
    "            for col in rpkm_df.columns:\n",
    "                rpkm_df[col] = rpkm_df[col] - control_avg\n",
    "            results.append({'Treatment': treatment, 'Avg_RPKM': rpkm_df.mean(axis=1).mean(), 'Std_RPKM': rpkm_df.mean().std()})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Plotting\n",
    "def get_color(sample_id):\n",
    "    '''\n",
    "    Set color for bar plots\n",
    "    '''\n",
    "    if \"SpRY\" in sample_id:\n",
    "        return '#9400D3'\n",
    "    elif \"WT\" in sample_id:\n",
    "        return '#A9A9A9'  \n",
    "    else:\n",
    "        return 'lightgrey'\n",
    "    \n",
    "def plot_bar(ax, xs, ys, title, ylabel, get_color, yerr=None, capsize=5):\n",
    "    x_pos = np.arange(len(xs))\n",
    "    colors = [get_color(sample_id) for sample_id in xs]\n",
    "    \n",
    "    if yerr is not None:\n",
    "        ax.bar(x_pos, ys, color=colors, yerr=yerr, capsize=capsize, \n",
    "               error_kw={'ecolor': 'black', 'linewidth': 1})\n",
    "    else:\n",
    "        ax.bar(x_pos, ys, color=colors)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(xs, rotation=90, fontsize=8)\n",
    "    ax.ticklabel_format(style='plain', axis='y')\n",
    "    ax.yaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "    ax.set_ylabel(ylabel)\n",
    "    \n",
    "    if yerr is not None:\n",
    "        ymax = max(np.array(ys) + np.array(yerr))\n",
    "        ax.set_ylim(0, ymax * 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f21eb5-5944-4d73-8986-104159a21750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SETUP ####\n",
    "\n",
    "# OUTPUT DIR\n",
    "out_dir = \"/groups/doudna/projects/mtrinidad_projects/ChIPSeq_HSKW/KMW_060324_ChIP/Pseudo_Analysis/\"\n",
    "\n",
    "# PEAK DIR\n",
    "peak_dir = f\"{out_dir}Peaks/\"\n",
    "peak_suffix = \"_peaks.narrowPeak\"\n",
    "\n",
    "# SAMPLE IDs: Fastq prefixes\n",
    "sample_ids = ['ChIP__KMW_060324_dSpRY_UCLA', \n",
    "              'ChIP__KMW_060324_dSpRYapo_UCLA',\n",
    "              'ChIP__KMW_060324_dWT_UCLA', \n",
    "              'ChIP__KMW_060324_dWTapo_UCLA'\n",
    "             ]\n",
    "\n",
    "## COLLECT MAPPED READ COUNTS FROM FULL-FASTQ ALIGNMENTS\n",
    "# FULL BAMS\n",
    "bam_suffix = \"_sorted.bam\"\n",
    "bam_dir = \"/groups/doudna/projects/mtrinidad_projects/ChIPSeq_HSKW/KMW_060324_ChIP/Alignments/\"\n",
    "bams = [bam_dir+x for x in os.listdir(bam_dir) if x.endswith(\"_sorted.bam\") and x.split(\"/\")[-1].replace(bam_suffix, \"\") in sample_ids]\n",
    "\n",
    "# Get mapping stats\n",
    "mapped_dict = {}\n",
    "for bam in bams:\n",
    "    base_id = bam.split(\"/\")[-1].replace(bam_suffix, \"\")\n",
    "    print(base_id)\n",
    "    mapped_reads = count_mapped_reads(bam)\n",
    "    mapped_dict[base_id] = mapped_reads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95abec3f-30d2-4714-93cb-361ed7215dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CREATE PSEUDO-REP ALIGNMENTS AND PEAKS: Create pseudoreps, align, dedup bams, Macs2 peak-calling ####\n",
    "\n",
    "## SET VARIABLES\n",
    "n_reps = 3 # Number pseudoreps\n",
    "threads='$SLURM_CPUS_ON_NODE'\n",
    "repair_dir = \"/shared/software/bbmap/latest/repair.sh\" # bbmap repair script to enforce fastq read order\n",
    "pseudo_bam_dir = f\"{out_dir}Pseudo_Alignments/\"\n",
    "dedup_dir = f\"{pseudo_bam_dir}/DEDUP/\"\n",
    "dedup_suffix = \"_nodups.bam\"\n",
    "\n",
    "# GENERATE SEEDS\n",
    "#seeds = [np.random.randint(100, 100 * (len(sample_ids)*n_reps+1)) for _ in range(0,(len(sample_ids)*n_reps+1))]\n",
    "seeds = [1496, 1729, 1875, 641, 627, 136, 101, 1370, 172, 1246, 1788, 1402, 942, 1488, 345, 1230, 1535, 611, 1292]\n",
    "print(f\"# SEEDS: {seeds}\")\n",
    "\n",
    "# Macs2 dir\n",
    "macs2_dir = \"/home/mtrinidad/miniconda3/envs/ChIP/bin/macs2\" \n",
    "\n",
    "# P-VAL\n",
    "p_val = 0.01\n",
    "\n",
    "# SCRIPT INFO\n",
    "script_name = f\"Create_PseudoReps_Align_and_Call_Peaks.sh\"\n",
    "bash_header = [\"#!/bin/bash\", \"#SBATCH -p memory\", \"#SBATCH --job-name pseudo\", \n",
    "               \"#SBATCH -o %j.out\", \"#SBATCH -e %j.err\", \"## ACTIVATE CONDA\", \n",
    "               'eval \"$(conda shell.bash hook)\"', 'conda activate ChIP',\n",
    "              ]\n",
    "\n",
    "# REFERENCES\n",
    "ref_dir = \"/groups/doudna/team_resources/shared_databases/human_reference_genomes/hg38/GENCODE_GRCh38p14_v44/GRCh38_PRI_bt\"\n",
    "\n",
    "## CREATE PSEUDOREPS\n",
    "pseudorep_bams = {}\n",
    "target_depth = 50000000.0\n",
    "\n",
    "# For each original BAM...\n",
    "for bam in bams:\n",
    "    base_id = bam.split(\"/\")[-1].replace(bam_suffix, \"\")\n",
    "    print(base_id)\n",
    "    temp_bams = []\n",
    "    bash_header.append(f\"## {base_id}:\")\n",
    "\n",
    "    \n",
    "    # CLEAN ORIGINAL BAM: Remove unaligned reads and format BAM\n",
    "    # Extract aligned reads only and store new bam\n",
    "    aligned_bam = f'{pseudo_bam_dir}{base_id}_aligned.bam'\n",
    "    filter_aligned_cmd = f'samtools view -@ {threads} -b -F 4 {bam} | samtools sort -@ {threads} -o {aligned_bam}'\n",
    "    bash_header.append(filter_aligned_cmd)\n",
    "\n",
    "    # Index clean alignment\n",
    "    index_cmd = f'samtools index -@ {threads} {aligned_bam}'\n",
    "    bash_header.append(index_cmd)\n",
    "\n",
    "    # Write mapped reads to Fastq\n",
    "    r1 = f'{pseudo_bam_dir}{base_id}_R1.fq.gz'\n",
    "    r2 = f'{pseudo_bam_dir}{base_id}_R2.fq.gz'\n",
    "    fastq_cmd = f'samtools fastq -@ {threads} -1 {r1} -2 {r2} {aligned_bam}'\n",
    "    bash_header.append(fastq_cmd)\n",
    "\n",
    "    # Repair read order in mapped-read fastq\n",
    "    r1_repaired = f'{pseudo_bam_dir}{base_id}_R1_repaired.fq.gz'\n",
    "    r2_repaired = f'{pseudo_bam_dir}{base_id}_R2_repaired.fq.gz'\n",
    "    repair_cmd = f'{repair_dir} in1={r1} in2={r2} out1={r1_repaired} out2={r2_repaired} repair'\n",
    "    bash_header.append(repair_cmd)\n",
    "\n",
    "    \n",
    "    # For each pseudo-rep...\n",
    "    for j in range(0, n_reps):\n",
    "        bash_header.append(f\"# {base_id} Pesudo {j}:\")\n",
    "        seed = seeds[j]\n",
    "        \n",
    "        # Get subsample fraction\n",
    "        fraction = target_depth/mapped_dict[base_id]\n",
    "\n",
    "        # Create pseudo-rep fastqs\n",
    "        r1_sub = f'{pseudo_bam_dir}{base_id}_pseudo{j}_R1.fq.gz'\n",
    "        r2_sub = f'{pseudo_bam_dir}{base_id}_pseudo{j}_R2.fq.gz'\n",
    "        \n",
    "        r1_sub_cmd = f'seqtk sample -s{seed} {r1_repaired} {fraction} | gzip > {r1_sub}'\n",
    "        bash_header.append(r1_sub_cmd)\n",
    "        \n",
    "        r2_sub_cmd = f'seqtk sample -s{seed} {r2_repaired} {fraction} | gzip > {r2_sub}'\n",
    "        bash_header.append(r2_sub_cmd)\n",
    "        \n",
    "        \n",
    "        # Create pseudo-rep BAM:\n",
    "        pseudoreplicate_bam = f\"{pseudo_bam_dir}{base_id}_pseudo{j}{bam_suffix}\"\n",
    "        bowtie_align = (f\"bowtie2 -x {ref_dir} -1 {r1_sub} -2 {r2_sub} -p {threads} \"\n",
    "                        f\"--no-discordant --local --no-mixed --maxins 1000 | \" # Minimize large gaps (confuses MACS2)\n",
    "                        f\"samtools view -bSh - | samtools sort -@ {threads} -o {pseudoreplicate_bam}\")\n",
    "        bash_header.append(bowtie_align)\n",
    "        \n",
    "        # Index pseudo-rep Bam\n",
    "        index_cmd = f'samtools index -@ {threads} {pseudoreplicate_bam}'\n",
    "        bash_header.append(index_cmd)\n",
    "\n",
    "        # Store bam\n",
    "        temp_bams.append(pseudoreplicate_bam)\n",
    "\n",
    "    # Clean up\n",
    "    clean_cmd = f'rm {r1} {r2} {aligned_bam}*'\n",
    "    bash_header.append(clean_cmd)\n",
    "    \n",
    "    # Store bams\n",
    "    pseudorep_bams[base_id] = temp_bams\n",
    "\n",
    "\n",
    "#### PEAK CALLING ####\n",
    "\n",
    "# BACKGROUND SAMPLES FOR MACS\n",
    "background_dict = {'ChIP__KMW_060324_dWT_UCLA':pseudorep_bams['ChIP__KMW_060324_dWTapo_UCLA'],\n",
    "                   'ChIP__KMW_060324_dSpRY_UCLA':pseudorep_bams['ChIP__KMW_060324_dSpRYapo_UCLA']\n",
    "                  }\n",
    "\n",
    "## CALL PEAKS AND DEDUP BAMS\n",
    "picard_dir = \"/shared/software/gatk/v4.1.5/bin/gatk\"\n",
    "all_ipd_bams = [bam for l in pseudorep_bams.values() for bam in l if \"apo\" not in bam]\n",
    "\n",
    "bash_header.append(f\"#### CALL PEAKS ####\")\n",
    "\n",
    "for bam in all_ipd_bams:\n",
    "    # Collect Vars\n",
    "    if \"apo\" in bam: # Skip Apo: double-check\n",
    "        continue\n",
    "    base = bam.split(\"/\")[-1].replace(\"_sorted.bam\", \"\").split(\"_pseudo\")[0] # For background sample lookup\n",
    "    sample_id = bam.split(\"/\")[-1].replace(\"_sorted.bam\", \"\") # For output naming\n",
    "    input_ctrl_bam = background_dict[base]\n",
    "\n",
    "    # CALL PEAKS\n",
    "    bash_header.append(f\"## MACS2 {sample_id}\")\n",
    "    print(f\"## {sample_id}\")\n",
    "    \n",
    "    macs2_peaks = (f\"{macs2_dir} callpeak -t {bam} -c {' '.join(input_ctrl_bam)} \"\n",
    "                   f\"-g hs -n {sample_id} -f BAM -B --SPMR -p {p_val} --outdir {peak_dir}  \"\n",
    "                   f\"2> {peak_dir}{sample_id}_macs2.log\")\n",
    "    bash_header.append(macs2_peaks)\n",
    "\n",
    "    # DEDUP\n",
    "    out_bam = bam.replace(\".bam\", \"_nodups.bam\").replace(bam_dir, dedup_dir)\n",
    "    metric_file = f\"{dedup_dir}{sample_id}_nodups_report.txt\"\n",
    "    rm_dup_cmd = (f\"{picard_dir} MarkDuplicates -I {bam} -O {out_bam} -M {metric_file} \"\n",
    "                  f\"--REMOVE_DUPLICATES true\")\n",
    "    print(rm_dup_cmd)\n",
    "    bash_header.append(rm_dup_cmd)\n",
    "\n",
    "\n",
    "# WRITE SCRIPT\n",
    "with open(script_name, \"w\") as f:\n",
    "    f.write(\"\\n\".join(bash_header))\n",
    "    \n",
    "#os.system(f'sbatch {script_name}') #JID: 499694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "577af6ae-5599-4e88-bed0-b9a3161b074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MAKE BIGWIGS ####\n",
    "## SETUP\n",
    "\n",
    "# BASH\n",
    "script_name = \"Pseudo_BigWigs.sh\"\n",
    "\n",
    "bash_header = [\n",
    "    \"#!/bin/bash\",\n",
    "    \"#SBATCH -p standard\",\n",
    "    \"#SBATCH --job-name pseudo_bw\",\n",
    "    \"#SBATCH -o %j.out\",\n",
    "    \"#SBATCH -e %j.err\",\n",
    "    \"## ACTIVATE CONDA\",\n",
    "    'eval \"$(conda shell.bash hook)\"',\n",
    "    'conda activate ChIP',\n",
    "]\n",
    "\n",
    "## SETUP COMMANDS\n",
    "#for bam in all_ipd_bams:\n",
    "for bam in [pseudo_bam_dir+x for x in os.listdir(pseudo_bam_dir) if x.endswith(bam_suffix)]:\n",
    "    sample_id = bam.split(\"/\")[-1].replace(\"_sorted.bam\", \"\") # For output naming\n",
    "    bash_header.append(f\"## {sample_id}\")\n",
    "    \n",
    "    # RAW DEDUP\n",
    "    dedup_raw_bw = f\"{pseudo_bam_dir}{sample_id}_RAW_DEDUP.bigWig\"\n",
    "    bw_raw_dedup = (f\"bamCoverage -b {bam} -o {dedup_raw_bw} \"\n",
    "                    f\"--numberOfProcessors {threads} --ignoreDuplicates --binSize 1\")\n",
    "    if not os.path.exists(dedup_raw_bw):\n",
    "        bash_header.append(bw_raw_dedup)    \n",
    "    else:\n",
    "        bash_header.append(\"#\"+bw_raw_dedup)\n",
    "\n",
    "    # CPM DEDUP\n",
    "    dedup_bw = f\"{pseudo_bam_dir}{sample_id}_CPM_Norm_DEDUP.bigWig\"\n",
    "    bw_dedup = (f\"bamCoverage -b {bam} -o {dedup_bw} --normalizeUsing CPM \"\n",
    "                f\"--numberOfProcessors {threads} --ignoreDuplicates --binSize 1\")\n",
    "    if not os.path.exists(dedup_bw):\n",
    "        bash_header.append(bw_dedup)\n",
    "    else:\n",
    "        bash_header.append(\"#\"+bw_dedup)\n",
    "\n",
    "    # RAW\n",
    "    raw_bw = f\"{pseudo_bam_dir}{sample_id}_RAW.bigWig\"\n",
    "    bw_cmd = (f\"bamCoverage -b {bam} -o {raw_bw} \"\n",
    "                f\"--numberOfProcessors {threads}  --binSize 1\")\n",
    "    if not os.path.exists(raw_bw):\n",
    "        bash_header.append(bw_cmd)\n",
    "    else:\n",
    "        bash_header.append(\"#\"+bw_cmd)\n",
    "        \n",
    "        \n",
    "# WRITE SCRIPT\n",
    "with open(script_name, \"w\") as f:\n",
    "    f.write(\"\\n\\n\".join(bash_header))\n",
    "\n",
    "#os.system(f'sbatch {script_name}') # JID: 499761, 500201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6658b66d-4f4a-4f0e-a667-61b29e71081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SNR ####\n",
    "'''\n",
    "Signal-to-Noise = num_reads_in_peaks / num_reads_outside_peaks\n",
    "'''\n",
    "\n",
    "narrowpeak_cols = [\"Chrom\", \"Start\", \"End\", \"Peak_Name\", \"Score\", \"Strand\", \"SignalValue\", \"pValue\", \"qValue\", \"Summit_Loc\"]\n",
    "spry_apo_dedups = [f'{dedup_dir}ChIP__KMW_060324_dSpRYapo_UCLA_pseudo{j}{dedup_suffix}' for j in range(0,3)]\n",
    "wt_apo_dedups = [f'{dedup_dir}ChIP__KMW_060324_dWTapo_UCLA_pseudo{j}{dedup_suffix}' for j in range(0,3)]\n",
    "\n",
    "bam_to_peak_dict = {f\"{dedup_dir}{sample_id}_pseudo{j}{dedup_suffix}\":f\"{peak_dir}{sample_id}_pseudo{j}{peak_suffix}\" \\\n",
    "                    for sample_id in sample_ids+['EMX1_gRNA3'] for j in range(0,3)}\n",
    "\n",
    "background_pseudo_dedup_dict = {'ChIP__KMW_060324_dSpRY_UCLA':spry_apo_dedups,\n",
    "                                'ChIP__KMW_060324_dWT_UCLA': wt_apo_dedups,\n",
    "                               }\n",
    "snrs = {}\n",
    "\n",
    "for bam,peak in bam_to_peak_dict.items():\n",
    "    if not os.path.exists(bam+\".bai\"):\n",
    "        os.system(f'samtools index -@ 12 {bam}')\n",
    "        \n",
    "    # Sample id\n",
    "    sample_id = bam.split(\"/\")[-1].replace(dedup_suffix, \"\")\n",
    "    print(f'## PROCESSING: {sample_id}')\n",
    "    \n",
    "    # Get Apo files\n",
    "    apo_bam_file = background_pseudo_dedup_dict[sample_id.split(\"_pseudo\")[0]]\n",
    "    print(\"##\", sample_id, apo_bam_file)\n",
    "\n",
    "    # READ SAMPLE PEAKS\n",
    "    sample_peaks_df = pd.read_csv(peak, sep='\\t', header=None, names=narrowpeak_cols)\n",
    "    \n",
    "    \n",
    "    # Calculate Signal-to-Noise ratio\n",
    "    signal_to_noise = calculate_signal_to_noise(bam, sample_peaks_df)\n",
    "    print(f\"Signal-to-Noise Ratio: {signal_to_noise}\")\n",
    "    snrs[sample_id] = signal_to_noise\n",
    "\n",
    "# Get Rep SNR means:\n",
    "wt_snr_rep_mean = np.mean([v for k,v in snrs.items() if \"_dWT_\" in k])*100\n",
    "spry_rep_mean = np.mean([v for k,v in snrs.items() if \"_dSpRY_\" in k])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "619845b9-e036-4387-82ad-5eb4b2db8dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IDR ####\n",
    "peak_rep_dict = {x:[f\"{peak_dir}{x}_pseudo{j}{peak_suffix}\" for j in range(0,3)] for x in background_dict.keys()}\n",
    "idr_dir = f\"{peak_dir}IDR/\"\n",
    "idr_threshold=0.05\n",
    "\n",
    "## CALCULATE IDR\n",
    "# BASH\n",
    "script_name = \"Pseudo_IDR.sh\"\n",
    "\n",
    "bash_header = [\n",
    "    \"#!/bin/bash\",\n",
    "    \"#SBATCH -p standard\",\n",
    "    \"#SBATCH --job-name pseudo_idr\",\n",
    "    \"#SBATCH -o %j.out\",\n",
    "    \"#SBATCH -e %j.err\",\n",
    "    \"## ACTIVATE CONDA\",\n",
    "    'eval \"$(conda shell.bash hook)\"',\n",
    "    'conda activate idr',\n",
    "]\n",
    "idr_results_files = []\n",
    "for treatment,rep_list in peak_rep_dict.items():\n",
    "    pairwise_combinations = itertools.combinations(rep_list, 2)\n",
    "\n",
    "    for pair in pairwise_combinations:\n",
    "        idr_results_file = f\"{idr_dir}{treatment}___{'_'.join([x.split('/')[-1].split(peak_suffix)[0] for x in pair])}\"\n",
    "        idr_results_files.append(idr_results_file)\n",
    "        \n",
    "        idr_cmd = (f\"idr --samples {' '.join(pair)} \"\n",
    "                   f\"--output-file {idr_results_file} \"\n",
    "                   f\"--plot \"\n",
    "                   f\"--idr-threshold {idr_threshold}\" \n",
    "                  )\n",
    "        \n",
    "        if not os.path.exists(idr_results_file):\n",
    "            bash_header.append(idr_cmd)\n",
    "        else:\n",
    "            bash_header.append(\"#\"+idr_cmd)\n",
    "\n",
    "# WRITE SCRIPT\n",
    "with open(script_name, \"w\") as f:\n",
    "    f.write(\"\\n\".join(bash_header))\n",
    "\n",
    "#os.system(f'sbatch {script_name}') # JID: 500668\n",
    "\n",
    "## COLLECT REPRODUCIBLE PEAK COUNTS\n",
    "idr_column_names = [\"chrom\",            # Chromosome name\n",
    "                    \"chromStart\",       # Start position of the peak on the chromosome\n",
    "                    \"chromEnd\",         # End position of the peak on the chromosome\n",
    "                    \"name\",             # Name assigned to the peak region\n",
    "                    \"score\",            # Scaled IDR score\n",
    "                    \"strand\",           # Strand information\n",
    "                    \"signalValue\",      # Measurement of enrichment for the region\n",
    "                    \"p-value\",          # Merged peak p-value\n",
    "                    \"q-value\",          # Merged peak q-value\n",
    "                    \"summit\",           # Position of the peak summit\n",
    "                    \"localIDR\",         # Local IDR score\n",
    "                    \"globalIDR\",        # Global IDR score\n",
    "                    \"rep1_chromStart\",  # Start position of the peak in replicate 1\n",
    "                    \"rep1_chromEnd\",    # End position of the peak in replicate 1\n",
    "                    \"rep1_signalValue\", # Signal value for replicate 1\n",
    "                    \"rep1_rank\",        # Rank of the peak in replicate 1\n",
    "                    \"rep2_chromStart\",  # Start position of the peak in replicate 2\n",
    "                    \"rep2_chromEnd\",    # End position of the peak in replicate 2\n",
    "                    \"rep2_signalValue\", # Signal value for replicate 2\n",
    "                    \"rep2_rank\"         # Rank of the peak in replicate 2\n",
    "                   ]\n",
    "\n",
    "reproducible_peak_counts_dict = {}\n",
    "treatments = []\n",
    "for idr_results in idr_results_files:\n",
    "    combo = idr_results.split(\"/\")[-1].replace(\"ChIP__KMW_060324_\",\"\").split(\"___\")[-1]\n",
    "    treatments.append(idr_results.split(\"/\")[-1].split(\"___\")[0])\n",
    "    idr_results_df = pd.read_csv(idr_results, sep = \"\\t\", header = None, names = idr_column_names)\n",
    "    reproducible_peak_counts_dict[combo] = idr_results_df.shape[0]\n",
    "\n",
    "reproducible_peak_counts_df = pd.DataFrame({\"Pair\":reproducible_peak_counts_dict.keys(), \n",
    "                                            \"N_Reproduced\":reproducible_peak_counts_dict.values(), \n",
    "                                            \"Treatment\":treatments})\n",
    "agg_df = reproducible_peak_counts_df.groupby(\"Treatment\").agg(Mean_Reproducible=(\"N_Reproduced\",\"mean\"), \n",
    "                                                              Std=(\"N_Reproduced\",\"std\")).reset_index()\n",
    "# Peak Counts (from IDR results files)\n",
    "peak_counts = {'dWT_UCLA_pseudo1': 44240,\n",
    "               'dWT_UCLA_pseudo0': 44633,\n",
    "               'dSpRY_UCLA_pseudo2': 45584,\n",
    "               'dSpRY_UCLA_pseudo1': 44744,\n",
    "               'dSpRY_UCLA_pseudo0': 45160,\n",
    "               'dWT_UCLA_pseudo2': 44297}\n",
    "peak_summary_df = reproducible_peak_counts_df.loc[(reproducible_peak_counts_df.Pair.str.contains(\"_dWT_\"))|\\\n",
    "                                          (reproducible_peak_counts_df.Pair.str.contains(\"_dSpRY_\"))]\n",
    "peak_summary_df[\"Rep1\"] = peak_summary_df[\"Pair\"].apply(lambda x: x.split(\"_dWT\")[0].split(\"_dSpRY\")[0])\n",
    "peak_summary_df[\"Rep2\"] = [\"dWT\", \"dWT\", \"dWT\", \"dSpRY\", \"dSpRY\", \"dSpRY\"] + peak_summary_df[\"Pair\"].apply(lambda x: x.split(\"_dWT\")[1] if \"_dWT\" in x else x.split(\"_dSpRY\")[1])\n",
    "peak_summary_df[\"Rep2\"] = peak_summary_df[\"Rep2\"]\n",
    "peak_summary_df[\"Rep_Rate\"] = 100*peak_summary_df[\"N_Reproduced\"]/[np.mean(y) for y in zip([peak_counts[x] for x in peak_summary_df[\"Rep1\"]],[peak_counts[x] for x in peak_summary_df[\"Rep2\"]])]\n",
    "\n",
    "peak_summary_df.groupby(\"Treatment\").agg(Avg_Rep_Rate=(\"Rep_Rate\", \"mean\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29f904d6-dd3f-431b-a098-581e3d1286d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RPKM: +/-25bp Guide with Seed-PAM Matches ####\n",
    "\n",
    "\n",
    "\n",
    "## REGEX SEED-PAM REGIONS\n",
    "# Seed&PAM Regular expressions:\n",
    "regex_dict = {\"SpRY\": [\"[ATGC]{15}AAGAA[ATGC][AG][ATGC]\",\n",
    "                       f'[ATGC][TC][ATGC]{reverse_complement(\"AAGAA\")}[ATGC]{{15}}' # Reverse complement \n",
    "                      ], \n",
    "              \"WT\":[\"[ATGC]{15}AAGAA[ATGC]GG\",\n",
    "                     f'CC[ATGC]{reverse_complement(\"AAGAA\")}[ATGC]{{15}}' # Reverse complement \n",
    "                   ]\n",
    "             }\n",
    "\n",
    "# Generate bed files of Seed&PAM matches\n",
    "match_dir = f\"{out_dir}Pattern_Matches/\"\n",
    "for cas,regex_pair in regex_dict.items():\n",
    "    find_hits_and_create_bed(cas,   \n",
    "                             regex_pair, \n",
    "                             bed_path = f\"{match_dir}{cas}_hits_SEEDPAM.bed\"\n",
    "                            )\n",
    "\n",
    "# Clean/sort beds for processing\n",
    "beds = [f\"{match_dir}WT_hits_SEEDPAM.bed\", f\"{match_dir}SpRY_hits_SEEDPAM.bed\"]\n",
    "new_bed_suffix = \"_sorted_v25bpWindow.bed\"\n",
    "clean_beds = []\n",
    "for bed in beds:\n",
    "    clean_bed = f'{bed.replace(\".bed\", new_bed_suffix)}'\n",
    "    clean_beds.append(clean_bed)\n",
    "    sort_bed_file(bed, clean_bed, window=25) # Sort bed and adjust for +/-25bp around Seed&PAM\n",
    "\n",
    "# Create bed file for intervals/regions WITHOUT Seed&PAM pattern matches\n",
    "faidx_path = \"/groups/doudna/team_resources/shared_databases/human_reference_genomes/hg38/GENCODE_GRCh38p14_v44/GRCh38_PRI.fa.fai\"\n",
    "no_match_suffix = \"_noSeedPAMMatch.bed\"\n",
    "no_seedpam_match_beds = []\n",
    "for bed in clean_beds:\n",
    "    no_match_bed = f'{bed.replace(\".bed\", no_match_suffix)}'\n",
    "    no_seedpam_match_beds.append(no_match_bed)\n",
    "    comp_cmd = f\"bedtools complement -i {bed} -g {faidx_path} > {no_match_bed}\"\n",
    "    os.system(comp_cmd)\n",
    "\n",
    "\n",
    "\n",
    "## GET COVERAGE FOR REGIONS OF INTEREST  \n",
    "def calculate_coverage(bam_file, bed_file, output_file=\"ROI_Coverage.bed\"):\n",
    "    '''\n",
    "    Get read coverage from bam for intervals within a bed file\n",
    "    '''\n",
    "    if not output_file:\n",
    "        output_file = f\"{os.path.splitext(bam_file)[0]}_coverage.bed\"\n",
    "    cmd = f\"bedtools coverage -a {bed_file} -b {bam_file} -counts > {output_file}\"\n",
    "    subprocess.run(cmd, shell=True)\n",
    "\n",
    "pseudorep_dedup_bam_list = [f'{dedup_dir}ChIP__KMW_060324_dWT_UCLA_pseudo2_sorted_nodups.bam',\n",
    "                            f'{dedup_dir}ChIP__KMW_060324_dSpRY_UCLA_pseudo2_sorted_nodups.bam',\n",
    "                            f'{dedup_dir}ChIP__KMW_060324_dWT_UCLA_pseudo1_sorted_nodups.bam',\n",
    "                            f'{dedup_dir}ChIP__KMW_060324_dSpRY_UCLA_pseudo1_sorted_nodups.bam',\n",
    "                            f'{dedup_dir}ChIP__KMW_060324_dWT_UCLA_pseudo0_sorted_nodups.bam',\n",
    "                            f'{dedup_dir}hIP__KMW_060324_dSpRY_UCLA_pseudo0_sorted_nodups.bam',\n",
    "                            f'{dedup_dir}ChIP__KMW_060324_dWTapo_UCLA_pseudo0_sorted_nodups.bam',\n",
    "                            f'{dedup_dir}ChIP__KMW_060324_dSpRYapo_UCLA_pseudo0_sorted_nodups.bam',\n",
    "                            f'{dedup_dir}ChIP__KMW_060324_dWTapo_UCLA_pseudo2_sorted_nodups.bam',\n",
    "                            f'{dedup_dir}ChIP__KMW_060324_dSpRYapo_UCLA_pseudo1_sorted_nodups.bam',\n",
    "                            f'{dedup_dir}DEDUP/ChIP__KMW_060324_dWTapo_UCLA_pseudo1_sorted_nodups.bam',\n",
    "                            f'{dedup_dir}ChIP__KMW_060324_dSpRYapo_UCLA_pseudo2_sorted_nodups.bam'\n",
    "                 ]\n",
    "hit_coverage = []\n",
    "nonhit_coverage = []\n",
    "hit_bed_cov_suffix = f'_coverage_hit_SEEDPAM.bed'\n",
    "nonhit_bed_cov_suffix = f'_NoMatchIntervals_SEEDPAM.bed'\n",
    "\n",
    "for bam in pseudorep_dedup_bam_list:\n",
    "\n",
    "    if \"SpRY\" in bam:\n",
    "        cas_beds = [x for x in no_seedpam_match_beds+clean_beds if \"SpRY\" in x]\n",
    "    else:\n",
    "        cas_beds = [x for x in no_seedpam_match_beds+clean_beds if \"WT\" in x]\n",
    "\n",
    "    for bed in cas_beds:\n",
    "        cas_id = bed.split(\"/\")[-1].split(\"_\")[0]\n",
    "        if bed.endswith(no_match_suffix):\n",
    "            out_cov = bam.replace(\".bam\", f'___{cas_id}PAM__'+nonhit_bed_cov_suffix)\n",
    "            nonhit_coverage.append(out_cov)\n",
    "        else:\n",
    "            out_cov = bam.replace(\".bam\", hit_bed_cov_suffix)\n",
    "            hit_coverage.append(out_cov)\n",
    "\n",
    "        calculate_coverage(bam, bed, output_file=out_cov)\n",
    "\n",
    "\n",
    "\n",
    "## Exclude BLACKLIST REGIONS\n",
    "'''\n",
    "Exclude blacklisted regions of hg38 from RPKM quantification\n",
    "'''\n",
    "\n",
    "blacklist = f\"{match_dir}hg38-blacklist.v2.bed\" # Blacklist file accroding to Methods\n",
    "\n",
    "for bed in glob.glob(f\"{dedup_dir}*coverage_hit_SEEDPAM.bed\")+\\\n",
    "           glob.glob(f\"{dedup_dir}*NoMatchIntervals_SEEDPAM.bed\"):\n",
    "   \n",
    "    out_bed = bed.replace(\".bed\", \"_filtered.bed\")\n",
    "    bedtools_cmd = f\"bedtools intersect -a {bed} -b {blacklist} -v > {out_bed}\"\n",
    "    if not os.path.exists(out_bed):\n",
    "        print(bedtools_cmd)\n",
    "\n",
    "\n",
    "\n",
    "## RUN RPKM CALCULATION: Use apo background subtraction\n",
    "# Collect Number mapped reads\n",
    "mapped_bam_reads = {}\n",
    "for bam in pseudorep_dedup_bam_list:\n",
    "    sample_id = bam.split(\"/\")[-1].split(\"_sorted_nodups.bam\")[0]\n",
    "    bamfile = pysam.AlignmentFile(bam, \"rb\")\n",
    "    total_reads = bamfile.mapped\n",
    "    bamfile.close()\n",
    "    mapped_bam_reads[sample_id] = total_reads\n",
    "    print(sample_id, total_reads)\n",
    "    \n",
    "experimental_setup = {'SpRY_SpRYPAM':['SpRY_SpRYPAM', 'SpRYapo_SpRYPAM'],  # Get RPKM for Seed&SpRyPAM regions subtract out Apo background\n",
    "                      'SpRY_WTPAM':['SpRY_WTPAM', 'SpRYapo_WTPAM'], # Get RPKM for Seed&WTPAM regions, ract out Apo background\n",
    "                      'WT_WTPAM': ['WT_WTPAM', 'WTapo_WTPAM'], # Get RPKM for Seed&WTPAM regions, subtract out Apo background\n",
    "                      'WT_SpRYPAM': ['WT_SpRYPAM', 'WTapo_SpRYPAM'] # Get RPKM for Seed&SpRYPAM regions, subtract out Apo background\n",
    "                     }\n",
    "# Get RPKM for Seed&PAM pattern\n",
    "SEEDPAM_hit_results = process_chip_data(f\"{dedup_dir}*coverage_hit_SEEDPAM_filtered.bed\", \n",
    "                                        mapped_bam_reads, experimental_setup)\n",
    "# Get RPKM for reverse-complement Seed&PAM pattern\n",
    "SEEDPAM_comp_results = process_chip_data(f\"{dedup_dir}*NoMatchIntervals_SEEDPAM_filtered.bed\",\n",
    "                                         mapped_bam_reads, experimental_setup)\n",
    "\n",
    "\n",
    "## PLOT\n",
    "hit_comp_pairs = {\"SEEDPAM\":[SEEDPAM_hit_results, SEEDPAM_comp_results]}\n",
    "xs = [\"SpRY_SpRYSeedPAM\", \"SpRY_NoSpRYSeedPAM\", \"SpRY_WTSeedPAM\", \"SpRY_NoWTSeedPAM\",\n",
    "      \"WT_WTSeedPAM\", \"WT_NoWTSeedPAM\", \"WT_SpRYSeedPAM\", \"WT_NoSpRYSeedPAM\"\n",
    "     ] # Format X-axis labels\n",
    "\n",
    "treatment_filter = ['SpRY_SpRYPAM','SpRY_SpRYPAM_NONE','WT_WTPAM','WT_WTPAM_NONE']\n",
    "\n",
    "for analysis_id,hit_comp in hit_comp_pairs.items():\n",
    "    comp = hit_comp[1]\n",
    "    all_data = pd.concat([hit_comp[0], comp])\n",
    "    all_data.sort_values(\"Treatment\", inplace = True)\n",
    "\n",
    "    fig, axs = plt.subplots(figsize=(3, 4))\n",
    "    plot_bar(axs, \n",
    "             all_data.Treatment.to_list(),\n",
    "             all_data[\"Avg_RPKM\"].to_list(), \n",
    "             \"\", \n",
    "             \"RPKM\",\n",
    "             get_color, \n",
    "             yerr=[0 if np.isnan(x) else x for x in all_data[\"Std_RPKM\"].to_list()]\n",
    "            )\n",
    "    \n",
    "    # Format Legend\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], color='mediumorchid', lw=4, label='SpRY'),\n",
    "        plt.Line2D([0], [0], color='grey', lw=4, label='WT')\n",
    "    ]\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.85])  # Adjust the margin\n",
    "    fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.94), ncol=3)\n",
    "    fig.suptitle(f\"{analysis_id.replace('173','100').replace('73','50')}\", fontsize=16, y=1.0)\n",
    "    \n",
    "    # Save\n",
    "    plt.savefig(f\"{analysis_id}bp_Clean.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6cf0e6-55a5-4766-b243-7b94faf27220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test_Py_3_9",
   "language": "python",
   "name": "test_py_3_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
